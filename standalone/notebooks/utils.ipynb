{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PM2.5: 35.9, AQI: 102\n",
      "PM2.5: 35.9, Measure Level: MeasureLevels.UNHEALTHY_FOR_SENSITIVE_GROUPS, Range Values: Min: 35.5, Max: 55.4\n",
      "AQI: 102, Measure Level: MeasureLevels.UNHEALTHY_FOR_SENSITIVE_GROUPS, Range Values: Min: 101, Max: 150\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import geohash2\n",
    "import pandas as pd\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['timezone'] = 'America/Bogota'\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "# Gloabl seaborn Theme\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "from io import StringIO\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats._result_classes import PearsonRResult\n",
    "from scipy.stats import NearConstantInputWarning, ConstantInputWarning\n",
    "from requests import Response\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Explicitly providing path to '.env'\n",
    "from pathlib import Path  # Python 3.6+ only\n",
    "# Load .env variables\n",
    "_ = load_dotenv(dotenv_path=f\"{Path().resolve().parents[1]}/standalone/.env\")\n",
    "\n",
    "# import import_ipynb\n",
    "# %run \"aqi_epa_pm25.ipynb\"\n",
    "# from aqi_epa_pm25 import PM25_MAX_VALUE, pm25_to_aqi\n",
    "\n",
    "# with the new api\n",
    "from importnb import imports\n",
    "with imports(\"ipynb\"):\n",
    "    from aqi_epa_pm25 import PM25_MAX_VALUE, pm25_to_aqi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_timestamp(datetime_iso8601: str) -> int:\n",
    "    \"\"\"\n",
    "    Datetime ISO 8601 Format to Timestamp\n",
    "    TZ='America/Bogota' -05:00\n",
    "\n",
    "    :params\n",
    "    :datetime_iso8601: str, Datetime ISO 8601 Format\n",
    "\n",
    "    :return: int, Timestamp\n",
    "\n",
    "    :example\n",
    "        - to_timestamp('2023-03-17T00:00:00-05:00')\n",
    "            return: 1679029200000\n",
    "    \"\"\"\n",
    "    return int(datetime.fromisoformat(datetime_iso8601).timestamp() * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_influxdb(sql_query: str) -> Response:\n",
    "    \"\"\"\n",
    "    Request to InfluxDB API REST\n",
    "\n",
    "    :params\n",
    "    :sql_query: str, InfluxDB SQL query\n",
    "\n",
    "    :return: Response, InfluxDB response as CSV text\n",
    "    \"\"\"\n",
    "    endpoint = os.getenv(\"URL_INFLUXDB_QUERY_ENDPOINT\", None)\n",
    "    database = os.getenv(\"DB_NAME_INFLUXDB\", None)\n",
    "    parameters = {\n",
    "        'db': database,\n",
    "        'q': sql_query,\n",
    "        'epoch': 'ms',\n",
    "        'format': 'json'\n",
    "    }\n",
    "    # To get response as CSV text\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    # GET Request\n",
    "    return requests.get(endpoint, params=parameters, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_tangaras(start_timestamp: int, end_timestamp: int) -> str:\n",
    "    \"\"\"\n",
    "    Get InfluxDB SQL query of all Tangara sensors that have reported data over a period of time.\n",
    "\n",
    "    :params:\n",
    "    :start_timestamp: int, timestamp datetime value, ms\n",
    "    :end_timestamp: int, timestamp datetime value, ms\n",
    "\n",
    "    :return: str, InfluxDB SQL Query\n",
    "    \"\"\"\n",
    "    # Period DateTime\n",
    "    period_time = f\"time >= {start_timestamp}ms AND time <= {end_timestamp}ms\"\n",
    "    # SQL\n",
    "    sql_query = \"SELECT DISTINCT(geo) AS \\\"geohash\\\" \"\\\n",
    "                \"FROM \\\"fixed_stations_01\\\" WHERE \"\\\n",
    "                \"(\\\"geo3\\\" = 'd29') AND \"\\\n",
    "                f\"{period_time} \"\\\n",
    "                \"GROUP BY \\\"name\\\";\"\n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_measure(mac_tangaras: [str], start_timestamp: int, end_timestamp: int, measure: str='pm25', group_by_time: str='30s') -> str:\n",
    "    \"\"\"\n",
    "    Get InfluxDB SQL query for specific measure (datatype) and for each Tangara sensor identified by MAC address between a period of time.\n",
    "\n",
    "    :params:\n",
    "    :mac_tangaras: [str], Tangara sensor MAC address\n",
    "    :start_timestamp: int, timestamp datetime value, ms\n",
    "    :end_timestamp: int, timestamp datetime value, ms\n",
    "    :measure: str, choice ['pm25', 'tmp', 'hum']\n",
    "    :group_by_time: str, choice ['30s', '1m', '1h']\n",
    "\n",
    "    :return: str, InfluxDB SQL Query\n",
    "    \"\"\"\n",
    "    # Period DateTime\n",
    "    period_time = f\"time >= {start_timestamp}ms AND time <= {end_timestamp}ms\"\n",
    "    # SQL Datatype by Tangara Sensor\n",
    "    sql_query = \"\"\n",
    "    head = \"last(\" if group_by_time == '30s' else \"mean(\"\n",
    "    for mac in mac_tangaras:\n",
    "        sql_query += f\"SELECT {head}\\\"{measure}\\\") \"\\\n",
    "                    \"FROM \\\"fixed_stations_01\\\" WHERE \"\\\n",
    "                    f\"(\\\"name\\\" = '{mac}') AND \"\\\n",
    "                    f\"{period_time} \" \\\n",
    "                    f\"GROUP BY time({group_by_time}) fill(null); \"\n",
    "    return sql_query[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_be_checked(df_sensor: DataFrame, threshold_data_percent: int=80) -> [bool, int, int]:\n",
    "    \"\"\"\n",
    "    Check if the sensor must be checked, because it has not reported enough data.\n",
    "    Return [bool, int]: [{Does it to be checked?}, {Total data}, {Total missing data}]\n",
    "\n",
    "    :params:\n",
    "    :df_sensor: DataFrame, Data reported by Tangara sensor\n",
    "    :threshold_data_percent: int, Threshold to check enough data reported\n",
    "\n",
    "    :return: [bool, int, int], Does it not report enough data?\n",
    "    \"\"\"\n",
    "    # Check missing data\n",
    "    total = df_sensor.shape[0]\n",
    "    missing_data_percent = round(df_sensor.isna().sum()[0] * 100 / total)\n",
    "    data_percent = round(df_sensor.count()[0] * 100 / total)\n",
    "    # Threshold\n",
    "    if data_percent < threshold_data_percent:\n",
    "        # to be checked\n",
    "        return [False, data_percent, missing_data_percent]\n",
    "    # OK\n",
    "    return [True, data_percent, missing_data_percent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_corr_ok(df_reference_sensor: DataFrame, df_target_sensor: DataFrame, threshold_corr: float=0.9) -> [bool, float]:\n",
    "    \"\"\"\n",
    "    Check if the target sensor must be checked, because it has not reference with the reference sensor.\n",
    "    Return [bool, float]: [{Is correlation ok?}, {Correlation percent}]\n",
    "\n",
    "    corr = 0, No correlation\n",
    "    corr = [-1, 0), Negative correlation\n",
    "    corr = (0, 1], Positive correlation\n",
    "\n",
    "    :params:\n",
    "    :df_reference_sensor: DataFrame, Reference Tangara sensor\n",
    "    :df_target_sensor: DataFrame, Target Tangara sensor\n",
    "    :threshold_corr: float, Threshold to check the positive correlation percent between both Tangara sensors\n",
    "\n",
    "    :return: [bool, float], There is not a correlation?\n",
    "    \"\"\"\n",
    "    # Pearson Correlation Coefficient\n",
    "    corr = 0\n",
    "    warnings.simplefilter(\"ignore\", category=NearConstantInputWarning)\n",
    "    warnings.simplefilter(\"ignore\", category=ConstantInputWarning)\n",
    "    if (not df_reference_sensor.hasnans and not df_target_sensor.hasnans) and (df_reference_sensor.shape[0] == df_target_sensor.shape[0]):\n",
    "        corr, _ = pearsonr(df_reference_sensor, df_target_sensor) if df_target_sensor.std() != 0 else PearsonRResult(0,0,alternative=0,n=0)\n",
    "        corr = 0 if math.isnan(corr) else corr\n",
    "        \n",
    "    # corr = 0, No correlation\n",
    "    # corr = [-1, 0), Negative correlation\n",
    "    # corr = (0, 1], Positive correlation\n",
    "    # Threshold\n",
    "    if corr < threshold_corr:\n",
    "        # There is not correlation\n",
    "        return [False, float(\"{:.2f}\".format(corr))]\n",
    "    # There is correlation\n",
    "    return [True, float(\"{:.2f}\".format(corr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_csv(df: DataFrame, filename: str, datafolder: str='0_raw') -> None:\n",
    "    \"\"\"\n",
    "    Save DataFrame into data folder as a CSV file.\n",
    "    datafolder: str, choice ['0_raw', '1_clean', '2_features', 'backup']\n",
    "\n",
    "    :params:\n",
    "    :df: DataFrame, pandas DataFrame\n",
    "    :filename: str, CSV file name with extension .csv\n",
    "    :datafolder: str, choice ['0_raw', '1_clean', '2_features', 'backup']\n",
    "    \"\"\"\n",
    "    # Save DataFrame into CSV file\n",
    "    path_datafolder=f\"{Path().resolve().parents[1]}/standalone/data/{datafolder}\"\n",
    "    df.to_csv(f\"{path_datafolder}/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_csv(filename: str, datafolder: str='0_raw', dtindex: bool=True) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load DataFrame from CSV file localted in data folder.\n",
    "    datafolder: str, choice ['0_raw', '1_clean', '2_features', 'backup']\n",
    "\n",
    "    :params:\n",
    "    :filename: str, CSV file name with extension .csv\n",
    "    :datafolder: str, choice ['0_raw', '1_clean', '2_features', 'backup']\n",
    "    :dtindex: bool, default True, Does the CSV file include and DATETIME column?\n",
    "    \n",
    "    :return: df: DataFrame, pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Load DataFrame from CSV file\n",
    "    path_csvfile=f\"{Path().resolve().parents[1]}/standalone/data/{datafolder}/{filename}\"\n",
    "    df_data = pd.read_csv(path_csvfile)\n",
    "    if dtindex:\n",
    "        df_data = df_data.set_index('DATETIME')\n",
    "        df_data.index = pd.to_datetime(df_data.index)\n",
    "        df_data = df_data.tz_convert(\"America/Bogota\")\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_tangara_sensors(start_timestamp: int, end_timestamp: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Get Data Frame Tangaras Sensors of all Tangara sensors that have reported data over a period of time.\n",
    "\n",
    "    :params:\n",
    "    :start_timestamp: int, timestamp datetime value, ms\n",
    "    :end_timestamp: int, timestamp datetime value, ms\n",
    "\n",
    "    :return: DataFrame, Tangaras Sensors\n",
    "    \"\"\"\n",
    "    # Query Tangaras\n",
    "    influxdb_sql_query_tangaras = query_tangaras(start_timestamp, end_timestamp)\n",
    "    # print(\"influxdb_sql_query_tangaras:\", influxdb_sql_query_tangaras)\n",
    "    # InfluxDB API REST Request\n",
    "    influxdb_request = request_influxdb(influxdb_sql_query_tangaras)\n",
    "    # print(\"influxdb_request:\", influxdb_request)\n",
    "    # print(\"influxdb_request.text:\", influxdb_request.text)\n",
    "\n",
    "    # Data Frame Tangaras\n",
    "    df_tangaras = pd.DataFrame([], columns=['ID','GEOHASH','MAC','GEOLOCATION','LATITUDE','LONGITUDE'])\n",
    "\n",
    "    # For each time series\n",
    "    for result in influxdb_request.json()['results']:\n",
    "        # print(\"result:\", result)\n",
    "        if 'series' in result:\n",
    "            for serie in result['series']:\n",
    "                # Get the serie\n",
    "                # print(\"serie:\", serie)\n",
    "\n",
    "                # DataFrame by statement_id and df_tangaras['ID']\n",
    "                tags = serie[\"tags\"]\n",
    "                columns = serie[\"columns\"]\n",
    "                values = serie[\"values\"]\n",
    "                # print(\"tags:\", tags)\n",
    "                # print(\"columns:\", columns)\n",
    "                # print(\"values:\", values)\n",
    "                df_tangara = pd.DataFrame(values, columns=columns)\n",
    "\n",
    "                # Remove/Add Columns\n",
    "                df_tangara['MAC'] = tags['name']\n",
    "                df_tangara['GEOLOCATION'] = df_tangara['geohash'].apply(lambda x: \" \".join(str(value) for value in list(geohash2.decode_exactly(x)[0:2])))\n",
    "                df_tangara['LATITUDE'] = df_tangara['GEOLOCATION'].apply(lambda x: x.split(' ')[0])\n",
    "                df_tangara['LONGITUDE'] = df_tangara['GEOLOCATION'].apply(lambda x: x.split(' ')[1])\n",
    "                df_tangara['ID'] = df_tangara['MAC'].apply(lambda x: f\"TANGARA_{x[-4:]}\")\n",
    "                df_tangara.rename(columns={'geohash': 'GEOHASH'}, inplace=True)\n",
    "                df_tangara = df_tangara[['ID','GEOHASH','MAC','GEOLOCATION','LATITUDE','LONGITUDE']]\n",
    "\n",
    "                # print('df_tangara.head():', df_tangara.head())\n",
    "\n",
    "                df_tangaras = pd.concat([df_tangaras, df_tangara], ignore_index=True)\n",
    "    \n",
    "    # Set Index\n",
    "    df_tangaras.set_index('ID', inplace=True)\n",
    "    \n",
    "    return df_tangaras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_data_sensors(df_tangaras: DataFrame, start_timestamp: int, end_timestamp: int, measure: str='pm25', group_by_time: str='30s') -> DataFrame:\n",
    "    \"\"\"\n",
    "    Get Measure Data Frame Sensors of all Tangara sensors that have reported data over a period of time.\n",
    "    \n",
    "    :params:\n",
    "    :df_tangaras: DataFrame, Tangaras DataFrame\n",
    "    :start_timestamp: int, timestamp datetime value, ms\n",
    "    :end_timestamp: int, timestamp datetime value, ms\n",
    "    :measure: str, choice ['pm25', 'tmp', 'hum']\n",
    "    :group_by_time: str, choice ['30s', '1m', '1h']\n",
    "\n",
    "    :return: DataFrame, Measure Data Frame Sensors\n",
    "    \"\"\"\n",
    "    # Data Frame Sensors List\n",
    "    df_sensors_list: [DataFrame] = []\n",
    "    # SQL Query Data Sensors\n",
    "    influxdb_sql_query_measure = query_measure(df_tangaras['MAC'].to_list(), start_timestamp, end_timestamp, measure, group_by_time)# print(\"influxdb_sql_query_measure:\", influxdb_sql_query_measure)\n",
    "    # print(\"influxdb_sql_query_measure:\", influxdb_sql_query_measure)\n",
    "    # InfluxDB API REST Request\n",
    "    influxdb_request = request_influxdb(influxdb_sql_query_measure)\n",
    "    # print(\"influxdb_request:\", influxdb_request)\n",
    "    # print(\"influxdb_request.text:\", influxdb_request.text)\n",
    "\n",
    "    # For each time series\n",
    "    for result in influxdb_request.json()['results']:\n",
    "        if 'series' in result:\n",
    "            # Get the series and the statement_id\n",
    "            series = result['series'][0]\n",
    "            statement_id = result.get('statement_id')\n",
    "            # print(\"series:\", series)\n",
    "            # print(\"statement_id:\", statement_id)\n",
    "\n",
    "            # DataFrame by statement_id and df_tangaras['ID']\n",
    "            columns = [\"DATETIME\", df_tangaras['ID'].to_list()[statement_id]]\n",
    "            values = series[\"values\"]\n",
    "            df_sensor = pd.DataFrame(values, columns=columns)\n",
    "            # Set Index on DATETIME\n",
    "            df_sensor.set_index('DATETIME', inplace=True)\n",
    "            # print('df_sensor.head():', df_sensor.head())\n",
    "\n",
    "            # Append to df_sensors_list\n",
    "            df_sensors_list.append(df_sensor)\n",
    "\n",
    "    # Join all df_sensors into a single DataFrame\n",
    "    df_sensors = df_sensors_list[0].join(df_sensors_list[1:]).reset_index()\n",
    "\n",
    "    # Date Time ISO 8601 Format, TZ='America/Bogota' -05:00\n",
    "    tz = timezone(timedelta(hours=-5))\n",
    "    df_sensors['DATETIME'] = df_sensors['DATETIME'].apply(lambda x: datetime.fromtimestamp(int(x) / 1000, tz=tz).isoformat())\n",
    "    df_sensors['DATETIME'] = pd.to_datetime(df_sensors['DATETIME'])\n",
    "\n",
    "    # SET GROUP BY TIME\n",
    "    GROUP_BY_TIME = {'30s': '30S', '1m': 'Min', '1h': 'H'}\n",
    "    group_by_time = GROUP_BY_TIME[group_by_time]\n",
    "\n",
    "    # Set Index\n",
    "    df_sensors.set_index('DATETIME', inplace=True)\n",
    "    df_sensors = df_sensors.asfreq(freq=group_by_time)\n",
    "    df_sensors = df_sensors.tz_convert(\"America/Bogota\")\n",
    "\n",
    "    df_sensors[df_sensors.columns.to_list()] = df_sensors[df_sensors.columns.to_list()].astype('float64')\n",
    "    \n",
    "    return df_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(df_data: DataFrame, num_v_charts: int=4) -> None:\n",
    "    \"\"\"\n",
    "    Plot Histograms\n",
    "    Each column fron df_data will be plotted as Histogramas in a subplots.\n",
    "\n",
    "    :params:\n",
    "    :df_data: DataFrame, each column in data frame will be plotted\n",
    "    :num_v_charts: int, default 4, how many colums the subplots will have\n",
    "    \"\"\"\n",
    "    # Canvas\n",
    "    num_cols = len(df_data.columns)\n",
    "    num_rows = math.ceil(len(df_data.columns)/num_v_charts)\n",
    "    fig, axes = plt.subplots(num_rows, num_v_charts, figsize=(22, 16), tight_layout=True, sharey=True)\n",
    "    fig.suptitle('Histograms', fontsize=16)\n",
    "    # Plot\n",
    "    k = 0\n",
    "    for i in range(0, num_rows):\n",
    "        for j in range(0, num_v_charts):\n",
    "            if k < num_cols:\n",
    "                sns.histplot(ax=axes[i, j], data=df_data[df_data.columns[k]], kde=True, discrete=False)\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxes(df_data: DataFrame, num_v_charts: int=4) -> None:\n",
    "    \"\"\"\n",
    "    Plot Boxplots\n",
    "    Each column fron df_data will be plotted as Boxplot in a subplots.\n",
    "\n",
    "    :params:\n",
    "    :df_data: DataFrame, each column in data frame will be plotted\n",
    "    :num_v_charts: int, default 4, how many colums the subplots will have\n",
    "    \"\"\"\n",
    "    # Canvas\n",
    "    num_cols = len(df_data.columns)\n",
    "    num_rows = math.ceil(len(df_data.columns)/num_v_charts)\n",
    "    fig, axes = plt.subplots(num_rows, num_v_charts, figsize=(22, 16), tight_layout=True)\n",
    "    fig.suptitle('Boxplots', fontsize=16)\n",
    "    # Plot\n",
    "    k = 0\n",
    "    for i in range(0, num_rows):\n",
    "        for j in range(0, num_v_charts):\n",
    "            if k < num_cols:\n",
    "                sns.boxplot(ax=axes[i, j], data=df_data[[df_data.columns[k]]], orient=\"h\", x=df_data.columns[k])\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(df_data: DataFrame, num_v_charts: int=4) -> None:\n",
    "    \"\"\"\n",
    "    Plot Lineplots\n",
    "    Each column fron df_data will be plotted as Lineplot in a subplots.\n",
    "\n",
    "    :params:\n",
    "    :df_data: DataFrame, each column in data frame will be plotted\n",
    "    :num_v_charts: int, default 4, how many colums the subplots will have\n",
    "    \"\"\"\n",
    "    # Canvas\n",
    "    num_cols = len(df_data.columns)\n",
    "    num_rows = math.ceil(len(df_data.columns)/num_v_charts)\n",
    "    fig, axes = plt.subplots(num_rows, num_v_charts, figsize=(22, 16), tight_layout=True, sharey=False)\n",
    "    fig.suptitle('Lineplots', fontsize=16)\n",
    "    # Plot\n",
    "    k = 0\n",
    "    for i in range(0, num_rows):\n",
    "        for j in range(0, num_v_charts):\n",
    "            if k < num_cols:\n",
    "                sns.lineplot(ax=axes[i, j], data=df_data[[df_data.columns[k]]], markers=True)\n",
    "                axes[i, j].tick_params(axis='x', rotation=30)\n",
    "                axes[i, j].set_ylabel(\"PM2.5\")\n",
    "                \n",
    "                locator = mdates.AutoDateLocator(minticks=3, maxticks=7)\n",
    "                formatter = mdates.ConciseDateFormatter(locator)\n",
    "                \n",
    "                axes[i, j].xaxis.set_major_locator(locator)\n",
    "                axes[i, j].xaxis.set_major_formatter(formatter)\n",
    "\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_pm25_outliers(df_pm25_raw: DataFrame) -> [DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Drop PM2.5 Outliers\n",
    "    Apply mask to drop outliers from PM2.5 values,\n",
    "    return a DataFrame without outliers and a resume dictionary by dataframe column\n",
    "\n",
    "    :params:\n",
    "    :df_pm25_raw: DataFrame, PM2.5 Raw Data Values\n",
    "\n",
    "    :return: [DataFrame, dict], Clean DataFrame and resume dictionary\n",
    "    \"\"\"\n",
    "    # Resume\n",
    "    resume = {}\n",
    "    # Each Data Sensor\n",
    "    for column in df_pm25_raw.columns:\n",
    "        # Apply mask to max value allowed, Max AQI Scale Value\n",
    "        df_pm25_raw[column] = df_pm25_raw[column].mask(df_pm25_raw[column] > PM25_MAX_VALUE, PM25_MAX_VALUE)\n",
    "        \n",
    "        # Standard Deviation\n",
    "        std = df_pm25_raw[column].std()\n",
    "        # Range = max value - min value\n",
    "        rango = df_pm25_raw[column].max() - df_pm25_raw[column].min()\n",
    "        # Quantiles\n",
    "        media = df_pm25_raw[column].median()\n",
    "        Q1 = df_pm25_raw[column].quantile(q=0.25)\n",
    "        Q2 = df_pm25_raw[column].quantile(q=0.50)\n",
    "        Q3 = df_pm25_raw[column].quantile(q=0.75)\n",
    "        min_val = df_pm25_raw[column].quantile(q=0)\n",
    "        max_val = df_pm25_raw[column].quantile(q=1.0)\n",
    "        # Interquartil Range\n",
    "        iqr = Q3 - Q1\n",
    "        # Limites para deteccion de outliers (solo aplica para datos simetricamente distribuidos)**\n",
    "        min_limit = Q1 - 1.5 * iqr\n",
    "        max_limit = Q3 + 1.5 * iqr\n",
    "        # Medidas de Tendencia Central**\n",
    "        resume[column] = {\n",
    "            \"std\": std,\n",
    "            \"rango\": rango,\n",
    "            \"media\": media,\n",
    "            \"Q1\": Q1,\n",
    "            \"Q2\": Q2,\n",
    "            \"Q3\": Q3,\n",
    "            \"min_val\": min_val,\n",
    "            \"max_val\": max_val,\n",
    "            \"iqr\": iqr,\n",
    "            \"min_lim\": min_limit,\n",
    "            \"max_lim\": max_limit\n",
    "        }\n",
    "\n",
    "        # Drop Outliers, using max_limit to apply mask and drop outliers\n",
    "        df_pm25_raw[column] = df_pm25_raw[column].mask(df_pm25_raw[column] > max_limit, None)\n",
    "    \n",
    "    return [df_pm25_raw, resume]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_pm25_to_df_aqi(df_pm25: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    PM2.5 DataFrame Sensors to AQI DataFrame Sensors\n",
    "\n",
    "    :params:\n",
    "    :df_pm25: DataFrame, PM2.5 DataFrame Sensors\n",
    "    \n",
    "    :return: DataFrame, AQI DataFrame Sensors\n",
    "    \"\"\"\n",
    "    aqi_values = df_pm25.copy()\n",
    "    for column in aqi_values.columns:\n",
    "        aqi_values[column] = aqi_values[column].apply(lambda x: pm25_to_aqi(x))\n",
    "        aqi_values[column] = aqi_values[column].astype('Int64')\n",
    "    return aqi_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
